
# Analysing Wildfire Data Using Chained Hadoop MapReduce Jobs Tutorial

Welcome to the GitHub repository for the tutorial on Analysing Wildfire Data Using Chained Hadoop MapReduce Jobs. This project aims to explore the relationship between historical weather and wildfire activities in Australia by leveraging the power of Hadoop MapReduce.

## Data and Research Question
The project utilises two datasets: Historical Wildfires and Historical Weather, obtained from the IBM wildfire data repository.  [Predicting wildfires with weather forecast data url: [(https://developer.ibm.com/exchanges/data/all/spot-challenge-wildfires/)](https://developer.ibm.com/exchanges/data/all/spot-challenge-wildfires/) .] The datasets cover seven regions in Australia and provide information on variables such as fire area, fire brightness, temperature, wind speed, radiation, and humidity. The research question we seek to answer is: "Do higher temperatures make wildfires stronger and cause larger burnt areas in any particular region?"

## Methodology
To answer the research question, we implement a series of four chained MapReduce jobs. These jobs involve filtering and extracting relevant variables, computing means, joining datasets, and calculating Pearson's correlation coefficients. By breaking down the problem into smaller tasks and chaining them together, we gain valuable insights into the impact of temperature on wildfire strength and burnt areas.

## MapReduce Design and Implementation
The project's design consists of four chained MapReduce jobs, orchestrated using the JobControl feature. Each job focuses on specific data processing steps, ensuring a modular and scalable approach. The chaining process is managed using ChainMapper and ChainReducer, allowing for seamless data flow between job stages.

The first two jobs process the Historical Weather and Historical Wildfires datasets separately, extracting and calculating relevant variables. The outputs of these jobs serve as inputs for the subsequent stages. The third job involves joining the processed datasets based on date and region, creating a unified dataset for further analysis. Finally, the fourth job calculates Pearson's correlation coefficients between temperature, fire area, and fire brightness.

<img src="https://github.com/Dima-Ramadan/Analysing-Wildfire-Data_Hadoop/assets/69980464/b2fed95d-56d0-423b-9270-2b71cfbb6b23" width="800" height = 600>

## Repository Structure
- `.java files/`: Contain the Java source codes for implementing the MapReduce jobs.
- `data_files/`: Contains input data for testing the MapReduce jobs.
- `output/`: Stores the output generated by the MapReduce jobs.
- `README.md`: Provides an overview of the project, methodology, and implementation details.
- `Wiki.md`: A step-by-step [tutorial](https://github.com/Dima-Ramadan/Analysing-Wildfire-Data_Hadoop/wiki) explaining the implementation of the chained MapReduce jobs.


Please refer to the tutorial for detailed instructions on running the MapReduce jobs and analysing wildfire data using chained Hadoop MapReduce.

Feel free to explore the code, datasets, and tutorial to gain insights into analysing wildfire data with Hadoop MapReduce.
Happy coding!
